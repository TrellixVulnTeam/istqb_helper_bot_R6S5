"""
Standard Glossary of Terms used in Software Testing
Version 3.2
Foundation (New) Terms
https://www.istqb.org/downloads/send/20-istqb-glossary/209-extract-of-terms-used-in-the-foundation-level-syllabus-2018.html
"""

GLOSSARY = {
"acceptance criteria" : "The criteria that a component or system must satisfy in order to be accepted by a user, customer, or other authorized entity.",
"acceptance testing" : "Formal testing with respect to user needs, requirements, and business processes conducted to determine whether or not a system satisfies the acceptance criteria and to enable the user, customers or other authorized entity to determine whether or not to accept the system.",
"accessibility" : "The degree to which a component or system can be used by people with the widest range of characteristics and capabilities to achieve a specified goal in a specified context of use.",
"accessibility testing" : "Testing to determine the ease by which users with disabilities can use a component or system.",
"actual result" : "The behavior produced/observed when a component or system is tested.",
"ad hoc reviewing" : "A review technique carried out by independent reviewers informally, without a structured process.",
"alpha testing" : "Simulated or actual operational testing conducted in the developer's test environment, by roles outside the development organization.",
"anomaly" : "Any condition that deviates from expectation based on requirements specifications, design documents, user documents, standards, etc., or from someone's perception or experience. Anomalies may be found during, but not limited to, reviewing, testing, analysis, compilation, or use of software products or applicable documentation.",
"audit" : "An independent examination of a work product, process, or set of processes that is performed by a third party to assess compliance with specifications, standards, contractual agreements, or other criteria.",
"availability" : "The degree to which a component or system is operational and accessible when required for use.",
"behavior" : "The response of a component or system to a set of input values and preconditions.",
"beta testing" : "Simulated or actual operational testing conducted at an external site, by roles outside the development organization.",
"black-box test technique" : "A procedure to derive and/or select test cases based on an analysis of the specification, either functional or non-functional, of a component or system without reference to its internal structure.",
"boundary value" : "A minimum or maximum value of an ordered equivalence partition.",
"boundary value analysis" : "A black-box test technique in which test cases are designed based on boundary values.",
"burndown chart" : "A publicly displayed chart that depicts the outstanding effort versus time in an iteration. It shows the status and trend of completing the tasks of the iteration. The X-axis typically represents days in the sprint, while the Y-axis is the remaining effort (usually either in ideal engineering hours or story points).",
"checklist-based reviewing" : "A review technique guided by a list of questions or required attributes.",
"checklist-based testing" : "An experience-based test technique whereby the experienced tester uses a high-level list of items to be noted, checked, or remembered, or a set of rules or criteria against which a product has to be verified.",
"code coverage" : "An analysis method that determines which parts of the software have been executed (covered) by the test suite and which parts have not been executed, e.g., statement coverage, decision coverage or condition coverage.",
"commercial off-the-shelf (COTS)" : "A software product that is developed for the general market, i.e. for a large number of customers, and that is delivered to many customers in identical format.",
"compatibility" : "The degree to which a component or system can exchange information with other components or systems.",
"complexity" : "The degree to which a component or system has a design and/or internal structure that is difficult to understand, maintain and verify.",
"compliance" : "The capability of the software product to adhere to standards, conventions or regulations in laws and similar prescriptions.",
"component" : "A minimal part of a system that can be tested in isolation.",
"component integration testing" : "Testing performed to expose defects in the interfaces and interactions between integrated components.",
"component specification" : "A description of a component's function in terms of its output values for specified input values under specified conditions, and required non-functional behavior (e.g., resourceutilization).",
"component testing" : "The testing of individual hardware or software components.",
"condition" : "A logical expression that can be evaluated as True or False, e.g., A>B.",
"configuration" : "The composition of a component or system as defined by the number, nature, and interconnections of its constituent parts.",
"configuration item" : "An aggregation of work products that is designated for configuration management and treated as a single entity in the configuration management process.",
"configuration management" : "A discipline applying technical and administrative direction and surveillance to identify and document the functional and physical characteristics of a configuration item, control changes to those characteristics, record and report change processing and implementation status, and verify compliance with specified requirements.",
"configuration management tool" : "A tool that provides support for the identification and control of configuration items, their status over changes and versions, and the release of baselines consisting of configuration items.",
"confirmation testing" : "Dynamic testing conducted after fixing defects with the objective to confirm that failures caused by those defects do not occur anymore.",
"contractual acceptance testing" : "Acceptance testing conducted to verify whether a system satisfies its contractual requirements.",
"control flow" : "The sequence in which operations are performed during the execution of a test item.",
"cost of quality" : "The total costs incurred on quality activities and issues and often split into prevention costs, appraisal costs, internal failure costs and external failure costs.",
"coverage" : "The degree to which specified coverage items have been determined or have been exercised by a test suite expressed as a percentage.",
"coverage item" : "An attribute or combination of attributes that is derived from one or more test conditions by using a test technique that enables the measurement of the thoroughness of the test execution.",
"coverage tool" : "A tool that provides objective measures of what structural elements, e.g., statements, branches have been exercised by a test suite.",
"data flow" : "An abstract representation of the sequence and possible changes of the state of data objects, where the state of an object is any of creation, usage, or destruction.",
"data-driven testing" : "A scripting technique that stores test input and expected results in a table or spreadsheet, so that a single control script can execute all of the tests in the table. Data-driven testing is often used to support the application of test execution tools such as capture/playback tools.",
"debugging" : "The process of finding, analyzing and removing the causes of failures in software.",
"decision" : "A type of statement in which a choice between two or more possible outcomes controls which set of actions will result.",
"decision coverage" : "The coverage of decision outcomes.",
"decision outcome" : "The result of a decision that determines the next statement to be executed.",
"decision table" : "A table used to show sets of conditions and the actions resulting from them.",
"decision table testing" : "A black-box test technique in which test cases are designed to execute the combinations of inputs and/or stimuli (causes) shown in a decision table.",
"decision testing" : "A white-box test technique in which test cases are designed to execute decision outcomes.",
"defect" : "An imperfection or deficiency in a work product where it does not meet its requirements or specifications.",
"defect density" : "The number of defects per unit size of a work product.",
"defect management" : "The process of recognizing and recording defects, classifying them, investigating them, taking action to resolve them, and disposing of them when resolved.",
"defect management tool" : "A tool that facilitates the recording and status tracking of defects.",
"defect report" : "Documentation of the occurrence, nature, and status of a defect.",
"driver" : "A software component or test tool that replaces a component that takes care of the control and/or the calling of a component or system.",
"dynamic analysis" : "The process of evaluating behavior, e.g., memory performance, CPU usage, of a system or component during execution.",
"dynamic analysis tool" : "A tool that provides run-time information on the state of the software code. These tools are most commonly used to identify unassigned pointers, check pointer arithmetic and to monitor the allocation, use and de-allocation of memory and to flag memory leaks.",
"dynamic testing" : "Testing that involves the execution of the software of a component or system.",
"effectiveness" : "Extent to which correct and complete goals are achieved.",
"efficiency" : "Resources expended in relation to the extent with which users achieve specified goals.",
"entry criteria" : "The set of conditions for officially starting a defined task.",
"equivalence partition" : "A portion of the value domain of a data element related to the test object for which all values are expected to be treated the same based on the specification.",
"equivalence partitioning" : "A black-box test technique in which test cases are designed to exercise equivalence partitions by using one representative member of each partition.",
"error" : "A human action that produces an incorrect result.",
"error guessing" : "A test technique in which tests are derived on the basis of the tester's knowledge of past failures, or general knowledge of failure modes.",
"executable statement" : "A statement which, when compiled, is translated into object code, and which will be executed procedurally when the program is running and may perform an action on data.",
"exercised" : "A program element is said to be exercised by a test case when the input value causes the execution of that element, such as a statement, decision, or other structural element.",
"exhaustive testing" : "A test approach in which the test suite comprises all combinations of input values and preconditions.",
"exit criteria" : "The set of conditions for officially completing a defined task.",
"expected result" : "The predicted observable behavior of a component or system executing under specified conditions, based on its specification or another source.",
"experience-based test technique" : "A procedure to derive and/or select test cases based on the tester's experience, knowledge and intuition.",
"experience-based testing" : "Testing based on the tester's experience, knowledge and intuition.",
"exploratory testing" : "An approach to testing whereby the testers dynamically design and execute tests based on their knowledge, exploration of the test item and the results of previous tests.",
"Extreme Programming (XP)" : "A software engineering methodology used within Agile software development whereby core practices are programming in pairs, doing extensive code review, unit testing of all code, and simplicity and clarity in code.",
"facilitator" : "The leader and main person responsible for an inspection or review process.",
"fail" : "A test is deemed to fail if its actual result does not match its expected result.",
"failure" : "An event in which a component or system does not perform a required function within specified limits.",
"failure rate" : "The ratio of the number of failures of a given category to a given unit of measure.",
"feature" : "An attribute of a component or system specified or implied by requirements documentation (for example reliability, usability or design constraints).",
"finding" : "A result of an evaluation that identifies some important issue, problem, or opportunity.",
"formal review" : "A form of review that follows a defined process with a formally documented output.",
"functional integration" : "An integration approach that combines the components or systems for the purpose of getting a basic functionality working early.",
"functional requirement" : "A requirement that specifies a function that a component or system must be able to perform.",
"functional suitability" : "The degree to which a component or system provides functions that meet stated and implied needs when used under specified conditions.",
"functional testing" : "Testing conducted to evaluate the compliance of a component or system with functional requirements.",
"GUI" : "Acronym for Graphical User Interface.",
"high-level test case" : "A test case without concrete values for input data and expected results.",
"IDEAL" : "An organizational improvement model that serves as a roadmap for initiating, planning, and implementing improvement actions. The IDEAL model is named for the five phases it describes: initiating, diagnosing, establishing, acting, and learning.",
"impact analysis" : "The identification of all work products affected by a change, including an estimate of the resources needed to accomplish the change.",
"incident report" : "Documentation of the occurrence, nature, and status of an incident.",
"incremental development model" : "A development lifecycle model in which the project scope is generally determined early in the project lifecycle, but time and cost estimates are routinely modified as the project team understanding of the product increases. The product is developed through a series of repeated cycles, each delivering an increment which successively adds to the functionality of the product.",
"independence of testing" : "Separation of responsibilities, which encourages the accomplishment of objective testing.",
"informal group review" : "An informal review performed by three or more persons.",
"informal review" : "A type of review without a formal (documented) procedure.",
"input" : "Data received by a component or system from an external source.",
"inspection" : "A type of formal review to identify issues in a work product, which provides measurement to improve the review process and the software development process.",
"installation guide" : "Supplied instructions on any suitable media, which guides the installer through the installation process. This may be a manual guide, step-by-step procedure, installation wizard, or any other similar process description.",
"integration" : "The process of combining components or systems into larger assemblies.",
"integration testing" : "Testing performed to expose defects in the interfaces and in the interactions between integrated components or systems.",
"interoperability" : "The degree to which two or more components or systems can exchange information and use the information that has been exchanged.",
"interoperability testing" : "Testing to determine the interoperability of a software product.",
"iterative development model" : "A development lifecycle where a project is broken into a usually large number of iterations. An iteration is a complete development loop resulting in a release (internal or external) of an executable product, a subset of the final product under development, which grows from iteration to iteration to become the final product.",
"keyword-driven testing" : "A scripting technique that uses data files to contain not only test data and expected results, but also keywords related to the application being tested. The keywords are interpreted by special supporting scripts that are called by the control script for the test.",
"lifecycle model" : "A description of the processes, workflows, and activities used in the development, delivery, maintenance, and retirement of a system.",
"load testing" : "A type of performance testing conducted to evaluate the behavior of a component or system under varying loads, usually between anticipated conditions of low, typical, and peak usage.",
"low-level test case" : "A test case with concrete values for input data and expected results.",
"maintainability" : "The degree to which a component or system can be modified by the intended maintainers.",
"maintenance" : "The process of modifying a component or system after delivery to correct defects, improve quality attributes, or adapt to a changed environment.",
"maintenance testing" : "Testing the changes to an operational system or the impact of a changed environment to an operational system.",
"master test plan" : "A test plan that is used to coordinate multiple test levels or test types.",
"maturity" : "(1) The capability of an organization with respect to the effectiveness and efficiency of its processes and work practices. (2) The degree to which a component or system meets needs for reliability under normal operation.",
"measure" : "The number or category assigned to an attribute of an entity by making a measurement.",
"measurement" : "The process of assigning a number or category to an entity to describe an attribute of that entity.",
"memory leak" : "A memory access failure due to a defect in a program's dynamic store allocation logic that causes it to fail to release memory after it has finished using it, eventually causing the program and/or other concurrent processes to fail due to lack of memory.",
"metric" : "A measurement scale and the method used for measurement.",
"milestone" : "A point in time in a project at which defined (intermediate) deliverables and results should be ready.",
"model-based testing (MBT)" : "Testing based on or involving models.",
"moderator" : "A neutral person who conducts a usability test session.",
"monitoring tool" : "A software tool or hardware device that runs concurrently with the component or system under test and supervises, records and/or analyzes the behavior of the component or system.",
"non-functional requirement" : "A requirement that describes how the component or system will do what it is intended to do.",
"non-functional testing" : "Testing conducted to evaluate the compliance of a component or system with non-functional requirements.",
"operational acceptance testing" : "Operational testing in the acceptance test phase, typically performed in a (simulated) operational environment by operations and/or systems administration staff focusing on operational aspects, e.g., recoverability, resource-behavior, installability and technical compliance.",
"operational environment" : "Hardware and software products installed at users' or customers' sites where the component or system under test will be used. The software may include operating systems, database management systems, and other applications.",
"output" : "Data transmitted by a component or system to an external destination.",
"pass" : "A test is deemed to pass if its actual result matches its expected result.",
"path" : "A sequence of events, e.g., executable statements, of a component or system from an entry point to an exit point.",
"peer review" : "A form of review of work products performed by others qualified to do the same work.",
"performance efficiency" : "The degree to which a component or system uses time, resources and capacity when accomplishing its designated functions.",
"performance indicator" : "A high-level metric of effectiveness and/or efficiency used to guide and control progressive development, e.g., lead-time slip for software development.",
"performance testing" : "Testing to determine the performance of a software product.",
"performance testing tool" : "A test tool that generates load for a designated test item and that measures and records its performance during test execution.",
"perspective-based reading" : "A review technique whereby reviewers evaluate the work product from different viewpoints.",
"planning poker" : "A consensus-based estimation technique, mostly used to estimate effort or relative size of user stories in Agile software development. It is a variation of the Wideband Delphi method using a deck of cards with values representing the units in which the team estimates.",
"portability" : "The ease with which the software product can be transferred from one hardware or software environment to another.",
"portability testing" : "Testing to determine the portability of a software product.",
"postcondition" : "The expected state of a test item and its environment at the end of test case execution.",
"precondition" : "The required state of a test item and its environment prior to test case execution.",
"priority" : "The level of (business) importance assigned to an item, e.g., defect.",
"probe effect" : "The effect on the component or system by the measurement instrument when the component or system is being measured, e.g., by a performance testing tool or monitor. For example performance may be slightly worse when performance testing tools are being used.",
"problem" : "An unknown underlying cause of one or more incidents.",
"process" : "A set of interrelated activities, which transform inputs into outputs.",
"process improvement" : "A program of activities designed to improve the performance and maturity of the organization's processes, and the result of such a program.",
"product risk" : "A risk impacting the quality of a product.",
"project" : "A project is a unique set of coordinated and controlled activities with start and finish dates undertaken to achieve an objective conforming to specific requirements, including the constraints of time, cost and resources.",
"project risk" : "A risk that impacts project success.",
"quality" : "The degree to which a component, system or process meets specified requirements and/or user/customer needs and expectations.",
"quality assurance" : "Part of quality management focused on providing confidence that quality requirements will be fulfilled.",
"quality characteristic" : "A category of product attributes that bears on quality.",
"quality control" : "The operational techniques and activities, part of quality management, that are focused on fulfilling quality requirements.",
"quality management" : "Coordinated activities to direct and control an organization with regard to quality. Direction and control with regard to quality generally includes the establishment of the quality policy and quality objectives, quality planning, quality control, quality assurance and quality improvement.",
"quality risk" : "A product risk related to a quality characteristic.",
"Rational Unified Process (RUP)" : "A proprietary adaptable iterative software development process framework consisting of four project lifecycle phases: inception, elaboration, construction and transition.",
"regression" : "A degradation in the quality of a component or system due to a change.",
"regression testing" : "Testing of a previously tested component or system following modification to ensure that defects have not been introduced or have been uncovered in unchanged areas of the software, as a result of the changes made.",
"regulatory acceptance testing" : "Acceptance testing conducted to verify whether a system conforms to relevant laws, policies and regulations.",
"reliability" : "The degree to which a component or system performs specified functions under specified conditions for a specified period of time.",
"reliability growth model" : "A model that shows the growth in reliability over time during continuous testing of a component or system as a result of the removal of defects that result in reliability failures.",
"requirement" : "A provision that contains criteria to be fulfilled.",
"requirements management tool" : "A tool that supports the recording of requirements, requirements attributes (e.g., priority, knowledge responsible) and annotation, and facilitates traceability through layers of requirements and requirements change management. Some requirements management tools also provide facilities for static analysis, such as consistency checking and violations to pre-defined requirements rules.",
"result" : "The consequence/outcome of the execution of a test. It includes outputs to screens, changes to data, reports, and communication messages sent out.",
"retrospective meeting" : "A meeting at the end of a project during which the project team members evaluate the project and learn lessons that can be applied to the next project.",
"review" : "A type of static testing during which a work product or process is evaluated by one or more individuals to detect issues and to provide improvements.",
"review plan" : "A document describing the approach, resources and schedule of intended review activities. It identifies, amongst others: documents and code to be reviewed, review types to be used, participants, as well as entry and exit criteria to be applied in case of formal reviews, and the rationale for their choice. It is a record of the review planning process.",
"reviewer" : "A participant in a review, who identifies issues in the work product.",
"risk" : "A factor that could result in future negative consequences.",
"risk analysis" : "The overall process of risk identification and risk assessment.",
"risk level" : "The qualitative or quantitative measure of a risk defined by impact and likelihood.",
"risk management" : "The coordinated activities to direct and control an organization with regard to risk.",
"risk mitigation" : "The process through which decisions are reached and protective measures are implemented for reducing or maintaining risks to specified levels.",
"risk type" : "A set of risks grouped by one or more common factors.",
"risk-based testing" : "Testing in which the management, selection, prioritization, and use of testing activities and resources are based on corresponding risk types and risk levels.",
"robustness" : "The degree to which a component or system can function correctly in the presence of invalid inputs or stressful environmental conditions.",
"role-based reviewing" : "A review technique where reviewers evaluate a work product from the perspective of different stakeholder roles.",
"root cause" : "A source of a defect such that if it is removed, the occurrence of the defect type is decreased or removed.",
"root cause analysis" : "An analysis technique aimed at identifying the root causes of defects. By directing corrective measures at root causes, it is hoped that the likelihood of defect recurrence will be minimized.",
"safety" : "The capability that a system will not, under defined conditions, lead to a state in which human life, health, property, or the environment is endangered.",
"scenario-based reviewing" : "A review technique where the review is guided by determining the ability of the work product to address specific scenarios.",
"scribe" : "A person who records information during the review meetings.",
"scrum" : "An iterative incremental framework for managing projects commonly used with Agile software development.",
"security" : "The degree to which a component or system protects information and data so that persons or other components or systems have the degree of access appropriate to their types and levels of authorization.",
"security testing" : "Testing to determine the security of the software product.",
"sequential development model" : "A type of development lifecycle model in which a complete system is developed in a linear way of several discrete and successive phases with no overlap between them.",
"session-based testing" : "An approach to testing in which test activities are planned as uninterrupted sessions of test design and execution, often used in conjunction with exploratory testing.",
"severity" : "The degree of impact that a defect has on the development or operation of a component or system.",
"simulation" : "The representation of selected behavioral characteristics of one physical or abstract system by another system.",
"simulator" : "A device, computer program or system used during testing, which behaves or operates like a given system when provided with a set of controlled inputs.",
"software" : "Computer programs, procedures, and possibly associated documentation and data pertaining to the operation of a computer system.",
"software development lifecycle" : "The activities performed at each stage in software development, and how they relate to one another logically and chronologically.",
"software lifecycle" : "The period of time that begins when a software product is conceived and ends when the software is no longer available for use. The software lifecycle typically includes a concept phase, requirements phase, design phase, implementation phase, test phase, installation and checkout phase, operation and maintenance phase, and sometimes, retirement phase. Note these phases may overlap or be performed iteratively.",
"software quality" : "The totality of functionality and features of a software product that bear on its ability to satisfy stated or implied needs.",
"specification" : "A document that specifies, ideally in a complete, precise and verifiable manner, the requirements, design, behavior, or other characteristics of a component or system, and, often, the procedures for determining whether these provisions have been satisfied.",
"stability" : "The degree to which a component or system can be effectively and efficiently modified without introducing defects or degrading existing product quality.",
"standard" : "Formal, possibly mandatory, set of requirements developed and used to prescribe consistent approaches to the way of working or to provide guidelines (e.g., ISO/IEC standards, IEEE standards, and organizational standards).",
"state diagram" : "A diagram that depicts the states that a component or system can assume, and shows the events or circumstances that cause and/or result from a change from one state to another.",
"state transition" : "A transition between two states of a component or system.",
"state transition testing" : "A black-box test technique using a state transition diagram or state table to derive test cases to evaluate whether the test item successfully executes valid transitions and blocks invalid transitions.",
"statement" : "An entity in a programming language, which is typically the smallest indivisible unit of execution.",
"statement coverage" : "The percentage of executable statements that have been exercised by a test suite.",
"statement testing" : "A white-box test technique in which test cases are designed to execute statements.",
"static analysis" : "The process of evaluating a component or system without executing it, based on its form, structure, content, or documentation.",
"static testing" : "Testing a work product without code being executed.",
"structural coverage" : "Coverage measures based on the internal structure of a component or system.",
"stub" : "A skeletal or special-purpose implementation of a software component, used to develop or test a component that calls or is otherwise dependent on it. It replaces a called component.",
"system" : "A collection of interacting elements organized to accomplish a specific function or set of functions.",
"System Integration Testing" : "Testing the combination and interaction of systems.",
"system testing" : "Testing an integrated system to verify that it meets specified requirements.",
"system under test (SUT)" : "A type of test object that is a system.",
"technical review" : "A formal review type by a team of technically-qualified personnel that examines the suitability of a work product for its intended use and identifies discrepancies from specifications and standards.",
"test" : "A set of one or more test cases.",
"test analysis" : "The activity that identifies test conditions by analyzing the test basis.",
"test approach" : "The implementation of the test strategy for a specific project.",
"test automation" : "The use of software to perform or support test activities, e.g., test management, test design, test execution and results checking.",
"test basis" : "The body of knowledge used as the basis for test analysis and design.",
"test case" : "A set of preconditions, inputs, actions (where applicable), expected results and postconditions, developed based on test conditions.",
"test case specification" : "Documentation of a set of one or more test cases.",
"test charter" : "Documentation of test activities in session-based exploratory testing.",
"test completion" : "The activity that makes test assets available for later use, leaves test environments in a satisfactory condition and communicates the results of testing to relevant stakeholders.",
"test condition" : "An aspect of the test basis that is relevant in order to achieve specific test objectives.",
"test control" : "A test management task that deals with developing and applying a set of corrective actions to get a test project on track when monitoring shows a deviation from what was planned.",
"test cycle" : "Execution of the test process against a single identifiable release of the test object.",
"test data" : "Data created or selected to satisfy the execution preconditions and inputs to execute one or more test cases.",
"test data preparation tool" : "A type of test tool that enables data to be selected from existing databases or created, generated, manipulated and edited for use in testing.",
"test design" : "The activity of deriving and specifying test cases from test conditions.",
"test design tool" : "A tool that supports the test design activity by generating test inputs from a specification that may be held in a CASE tool repository, e.g., requirements management tool, from specified test conditions held in the tool itself, or from code.",
"test environment" : "An environment containing hardware, instrumentation, simulators, software tools, and other support elements needed to conduct a test.",
"test estimation" : "The calculated approximation of a result related to various aspects of testing (e.g., effort spent, completion date, costs involved, number of test cases, etc.) which is usable even if input data may be incomplete, uncertain, or noisy.",
"test execution" : "The process of running a test on the component or system under test, producing actual result(s).",
"test execution schedule" : "A schedule for the execution of test suites within a test cycle.",
"test execution tool" : "A test tool that executes tests against a designated test item and evaluates the outcomes against expected results and postconditions.",
"test harness" : "A test environment comprised of stubs and drivers needed to execute a test.",
"test implementation" : "The activity that prepares the testware needed for test execution based on test analysis and design.",
"test infrastructure" : "The organizational artifacts needed to perform testing, consisting of test environments, test tools, office environment and procedures.",
"test input" : "The data received from an external source by the test object during test execution. The external source can be hardware, software or human.",
"test item" : "A part of a test object used in the test process.",
"test leader" : "On large projects, the person who reports to the test manager and is responsible for project management of a particular test level or a particular set of testing activities.",
"test level" : "A specific instantiation of a test process.",
"test management" : "The planning, scheduling, estimating, monitoring, reporting, control and completion of test activities.",
"test management tool" : "A tool that provides support to the test management and control part of a test process. It often has several capabilities, such as testware management, scheduling of tests, the logging of results, progress tracking, incident management and test reporting.",
"test manager" : "The person responsible for project management of testing activities and resources, and evaluation of a test object. The individual who directs, controls, administers, plans and regulates the evaluation of a test object.",
"test monitoring" : "A test management activity that involves checking the status of testing activities, identifying any variances from the planned or expected status, and reporting status to stakeholders.",
"test object" : "The component or system to be tested.",
"test objective" : "A reason or purpose for designing and executing a test.",
"test oracle" : "A source to determine expected results to compare with the actual result of the system under test.",
"test plan" : "Documentation describing the test objectives to be achieved and the means and the schedule for achieving them, organized to coordinate testing activities.",
"test planning" : "The activity of establishing or updating a test plan.",
"test policy" : "A high-level document describing the principles, approach and major objectives of the organization regarding testing.",
"test procedure" : "A sequence of test cases in execution order, and any associated actions that may be required to set up the initial preconditions and any wrap up activities post execution.",
"test process" : "The set of interrelated activities comprising of test planning, test monitoring and control, test analysis, test design, test implementation, test execution, and test completion.",
"test process improvement" : "A program of activities designed to improve the performance and maturity of the organization's test processes and the results of such a program.",
"test progress report" : "A test report produced at regular intervals about the progress of test activities against a baseline, risks, and alternatives requiring a decision.",
"test report" : "Documentation summarizing test activities and results.",
"test reporting" : "Collecting and analyzing data from testing activities and subsequently consolidating the data in a report to inform stakeholders.",
"test schedule" : "A list of activities, tasks or events of the test process, identifying their intended start and finish dates and/or times, and interdependencies.",
"test script" : "A sequence of instructions for the execution of a test.",
"test session" : "An uninterrupted period of time spent in executing tests. In exploratory testing, each test session is focused on a charter, but testers can also explore new opportunities or issues during a session. The tester creates and executes on the fly and records their progress.",
"test strategy" : "Documentation that expresses the generic requirements for testing one or more projects run within an organization, providing detail on how testing is to be performed, and is aligned with the test policy.",
"test suite" : "A set of test cases or test procedures to be executed in a specific test cycle.",
"test summary report" : "A test report that provides an evaluation of the corresponding test items against exit criteria.",
"test technique" : "A procedure used to derive and/or select test cases.",
"test tool" : "A software product that supports one or more test activities, such as planning and control, specification, building initial files and data, test execution and test analysis.",
"test type" : "A group of test activities based on specific test objectives aimed at specific characteristics of a component or system.",
"testability" : "The degree of effectiveness and efficiency with which tests can be designed and executed for a component or system.",
"testable requirement" : "A requirements that is stated in terms that permit establishment of test designs (and subsequently test cases) and execution of tests to determine whether the requirement has been met.",
"tester" : "A skilled professional who is involved in the testing of a component or system.",
"testing" : "The process consisting of all lifecycle activities, both static and dynamic, concerned with planning, preparation and evaluation of software products and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.",
"testware" : "Work products produced during the test process for use in planning, designing, executing, evaluating and reporting on testing.",
"traceability" : "The degree to which a relationship can be established between two or more work products.",
"understandability" : "The capability of the software product to enable the user to understand whether the software is suitable, and how it can be used for particular tasks and conditions of use.",
"unit test framework" : "A tool that provides an environment for unit or component testing in which a component can be tested in isolation or with suitable stubs and drivers. It also provides other support for the developer, such as debugging capabilities.",
"unreachable code" : "Code that cannot be reached and therefore is impossible to execute.",
"usability" : "The degree to which a component or system can be used by specified users to achieve specified goals in a specified context of use.",
"usability testing" : "Testing to evaluate the degree to which the system can be used by specified users with effectiveness, efficiency and satisfaction in a specified context of use.",
"use case" : "A sequence of transactions in a dialogue between an actor and a component or system with a tangible result, where an actor can be a user or anything that can exchange information with the system.",
"use case testing" : "A black-box test technique in which test cases are designed to execute scenarios of use cases.",
"user acceptance testing" : "Acceptance testing conducted in a real or simulated operational environment by intended users focusing their needs, requirements and business processes.",
"user interface" : "All components of a system that provide information and controls for the user to accomplish specific tasks with the system.",
"user story" : "A high-level user or business requirement commonly used in Agile software development, typically consisting of one sentence in the everyday or business language capturing what functionality a user needs and the reason behind this, any non-functional criteria, and also includes acceptance criteria.",
"V-model" : "A sequential development lifecycle model describing a one-for-one relationship between major phases of software development from business requirements specification to delivery, and corresponding test levels from acceptance testing to component testing.",
"validation" : "Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.",
"variable" : "An element of storage in a computer that is accessible by a software program by referring to it by a name.",
"verification" : "Confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.",
"walkthrough" : "A type of review in which an author leads members of the review through a work product and the members ask questions and make comments about possible issues.",
"white-box test technique" : "A procedure to derive and/or select test cases based on an analysis of the internal structure of a component or system.",
"white-box testing" : "Testing based on an analysis of the internal structure of the component or system.",
"Wideband Delphi" : "An expert-based test estimation technique that aims at making an accurate estimation using the collective wisdom of the team members."
}